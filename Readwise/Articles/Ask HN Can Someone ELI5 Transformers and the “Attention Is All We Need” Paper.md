---
Type: articles
tags:
  - Readwise
Author: Sai_
URL: https://news.ycombinator.com/item?id=35977891
Summary: I have zero AI/ML knowledge but Steve Yegge on Medium thinks that the team behind Transformers deserves a Nobel.Makes me want to better understand this tech.Edit:thank you for some amazing top level responses and links to valuable content on this subject.
Comments URL: https://news.ycombinator.com/item?id=35977891
Points: 255
Related: 
Processed: false
owner: Tanzeel159
repo: Digital-Garden-Content
attachment: true
dataview: false
share: true
date created: 2024-10-20 08:11:34
date modified: 2024-10-20 10:58:34
---
![rw-book-cover](https://news.ycombinator.com/favicon.ico)

## Highlights
- Imagine you're in school, and your teacher asks you and your friends to work on a group project. Each of you has a different set of skills, and you need to work together to complete the project successfully. In this analogy, the group project is like a sentence or a piece of text that a language model is trying to understand. ([View Highlight](https://read.readwise.io/read/01h257wjsk9yqjj6ct8f2xtkmc))
- Transformers are a type of model that helps computers understand and generate text, like in our group project. ([View Highlight](https://read.readwise.io/read/01h257wsdg54xj6d2b4z14eh99))
- The key idea behind transformers is something called "attention." ([View Highlight](https://read.readwise.io/read/01h257wyv3xsa3kwsq26jjapv9))
- Attention helps the model figure out which words in a sentence are the most important to focus on ([View Highlight](https://read.readwise.io/read/01h257x59jteby6b0prz7a3yqf))
- Instead of focusing on each word one at a time, transformers can look at all the words together, decide which ones are the most important, and use that information to understand the text better. ([View Highlight](https://read.readwise.io/read/01h257xvgv0rmzhg651c60rkhn))
